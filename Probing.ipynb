{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Probing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "221EpIl82ACD",
        "cellView": "form"
      },
      "source": [
        "# @title Install Transformers and Retrieve Probing Data\n",
        "from IPython.display import clear_output\n",
        "!pip install transformers\n",
        "!git clone https://github.com/facebookresearch/SentEval\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "WqxC1_f66w3h"
      },
      "source": [
        "# @title Import Requirements\n",
        "\n",
        "from transformers import (\n",
        "    BertConfig,\n",
        "    BertTokenizer,\n",
        "    TFBertModel,\n",
        "    BertModel,\n",
        "    glue_processors,\n",
        "    glue_convert_examples_to_features,\n",
        "    set_seed\n",
        ")\n",
        "from transformers.optimization_tf import create_optimizer\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import copy \n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.nn as nn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPzzfc3W_KN",
        "cellView": "form"
      },
      "source": [
        "# @title Hyperparameters\n",
        "BATCH_SIZE =  32# @param {type:\"integer\"}\n",
        "EPOCHS =  10#@param {type:\"integer\"}\n",
        "MAX_LENGTH =   64#@param {type:\"integer\"}\n",
        "\n",
        "TASK = \"qqp\" #@param [\"cola\", \"sst2\", \"mrpc\", \"sts\", \"qqp\", \"mnli\", \"qnli\", \"rte\"]\n",
        "\n",
        "MODEL_SIZE = \"full\"#@param [\"full\", \"7k\", \"2.5k\", \"1k\"]\n",
        "PROBE = \"BigramShift\" #@param ['Length','BigramShift', 'TopConst', 'Tense','SubjNumber', 'ObjNumber', 'OddManOut', 'CoordinationInversion']\n",
        "MODEL_SEED = \"42\" #@param [42, 123, 1234]\n",
        "PROBE_SEED = \"60\" #@param [40, 50, 60]\n",
        "num_labels = 2\n",
        "\n",
        "if PROBE == \"TopConst\":\n",
        "  num_labels = 20\n",
        "elif PROBE == 'Length':\n",
        "    num_labels = 6\n",
        "\n",
        "LEARNING_RATE =  3e-4 #@param {type:\"number\"}\n",
        "WARMUP_RATIO =   0.1 #@param {type:\"number\"}\n",
        "LAYER = \"12\" #@param [1,2,3,4, 5,6, 7,8, 9,10, 11, 12] \n",
        "LAYER = int(LAYER)\n",
        "\n",
        "# Loading fine-tuned models. Replace this line with the path to your saved models directory\n",
        "SAVED_MODELS_DIR = \"/content/drive/MyDrive/Fine-Tuned-Models/\" + TASK + '-' + MODEL_SIZE + '-' + str(MODEL_SEED)\n",
        "\n",
        "# List of tasks\n",
        "DATA_NAME = \"\"\n",
        "if PROBE == \"Length\":\n",
        "  DATA_NAME = \"sentence_length.txt\"\n",
        "elif PROBE == \"BigramShift\":\n",
        "  DATA_NAME = \"bigram_shift.txt\"\n",
        "elif PROBE == \"TopConst\":\n",
        "  DATA_NAME = \"top_constituents.txt\"\n",
        "elif PROBE == \"Tense\":\n",
        "  DATA_NAME = \"past_present.txt\"\n",
        "elif PROBE == \"SubjNumber\":\n",
        "  DATA_NAME = \"subj_number.txt\"\n",
        "elif PROBE == \"ObjNumber\":\n",
        "  DATA_NAME = \"obj_number.txt\"\n",
        "elif PROBE == \"OddManOut\":\n",
        "  DATA_NAME = \"odd_man_out.txt\"\n",
        "elif PROBE == \"CoordinationInversion\":\n",
        "  DATA_NAME = \"coordination_inversion.txt\"\n",
        "\n",
        "set_seed(int(PROBE_SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5ql0uu6kPoH",
        "cellView": "form"
      },
      "source": [
        "# @title Loading Data Set\n",
        "df = pd.read_csv(\"/content/SentEval/data/probing/\" + DATA_NAME, sep = '\\t', header=None)\n",
        "\n",
        "if PROBE != 'Length':\n",
        "  df[1] = df[1].factorize()[0]\n",
        "\n",
        "df_train = df[df[0]==\"tr\"]\n",
        "df_val = df[df[0]==\"va\"]\n",
        "df_test = df[df[0]==\"te\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTAmWbYJ580Y",
        "cellView": "form"
      },
      "source": [
        "# @title Probe model\n",
        "\n",
        "class ProbeModel(tf.keras.Model):\n",
        "  def __init__(self, bert_model, num_labels, layer, *inputs, **kwargs):\n",
        "    super(ProbeModel, self).__init__(name=\"ProbeModel\")\n",
        "    self.bert = bert_model\n",
        "    self.bert.trainable = False\n",
        "    self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "    self.layer = layer\n",
        "    self.classifier = tf.keras.layers.Dense(\n",
        "                      num_labels,\n",
        "                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
        "                      name=\"classifier\")\n",
        "    \n",
        "  \n",
        "  def call(self, inputs, **kwargs):\n",
        "\n",
        "    outputs = self.bert(inputs, **kwargs)\n",
        "    pooled_out = outputs[2][self.layer]\n",
        "    pooled_out = pooled_out[:,0,:]\n",
        "    # pooled_out = outputs[1]\n",
        "    \n",
        "    droped_out = self.dropout(pooled_out, training=kwargs.get(\"training\", False))\n",
        "    output = self.classifier(droped_out)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSRtIriB9SIP",
        "cellView": "form"
      },
      "source": [
        "# @title Tokenizer\n",
        "def tokenize(df, tokenizer, MAX_LENGTH):\n",
        "    input_ids, input_masks, input_segments = [],[],[]\n",
        "    for i in range(len(df)):\n",
        "        inputs = tokenizer.encode_plus(df[2].iloc[i], add_special_tokens=True, \n",
        "                                       return_attention_mask=True, return_token_type_ids=True, max_length = MAX_LENGTH,\n",
        "                                       pad_to_max_length=True, truncation =True )\n",
        "\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])  \n",
        "\n",
        "    return [tf.cast(input_ids,tf.int32),tf.cast(input_masks,tf.int32), tf.cast(input_segments,tf.int32)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pd-Mz7IMYEy",
        "cellView": "form"
      },
      "source": [
        "# @title Load Model\n",
        "\n",
        "casing = \"bert-base-uncased\" \n",
        "config = BertConfig.from_pretrained(casing, num_labels=num_labels)\n",
        "config.output_hidden_states = True\n",
        "tokenizer = BertTokenizer.from_pretrained(casing)\n",
        "\n",
        "bert_model = TFBertModel.from_pretrained(SAVED_MODELS_DIR, from_pt=True, config = config)\n",
        "bert_model.output_hidden_states=True\n",
        "model = ProbeModel(bert_model, num_labels, LAYER)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV3izjIm4DNn",
        "cellView": "form"
      },
      "source": [
        "# @title Make dataset ready \n",
        "\n",
        "train_dataset = tokenize(df_train, tokenizer, MAX_LENGTH)\n",
        "valid_dataset = tokenize(df_val, tokenizer, MAX_LENGTH)\n",
        "test_dataset  = tokenize(df_test, tokenizer, MAX_LENGTH)\n",
        "  \n",
        "\n",
        "train_steps = int(np.ceil(len(df_train) / BATCH_SIZE))\n",
        "valid_steps = int(np.ceil((len(df_val)) / BATCH_SIZE))\n",
        "test_steps = int(np.ceil(len(df_test) / BATCH_SIZE))\n",
        "\n",
        "clear_output()\n",
        "print(\"completed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "yebPoFtH_GKT"
      },
      "source": [
        "# @title Freeze BERT\n",
        "model.bert.bert.embeddings.trainable = False\n",
        "model.bert.bert.pooler.trainable = False\n",
        "for i in range(config.num_hidden_layers):\n",
        "  model.bert.bert.encoder.layer[i].attention.self_attention.trainable = False\n",
        "  model.bert.bert.encoder.layer[i].attention.dense_output.dense.trainable = False\n",
        "  model.bert.bert.encoder.layer[i].attention.dense_output.LayerNorm.trainable = False\n",
        "  model.bert.bert.encoder.layer[i].intermediate.trainable = False\n",
        "  model.bert.bert.encoder.layer[i].bert_output.dense.trainable = False\n",
        "  model.bert.bert.encoder.layer[i].bert_output.LayerNorm.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_tNzePP-i28",
        "cellView": "form"
      },
      "source": [
        "# @title Metrics, Loss & Optimizer\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "opt, scheduler = create_optimizer(init_lr=LEARNING_RATE,\n",
        "                       num_train_steps=train_steps * EPOCHS,\n",
        "                       num_warmup_steps=int(train_steps * EPOCHS * WARMUP_RATIO),\n",
        "                       adam_epsilon=1e-6,\n",
        "                       weight_decay_rate = 0)\n",
        "optimizer = tf.keras.optimizers.Adam(3e-4)\n",
        "model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D49nsl1JL6Rz",
        "cellView": "form"
      },
      "source": [
        "# @title Callback\n",
        "class ModelCheckpoint(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, monitor, save_path):\n",
        "    super(ModelCheckpoint, self).__init__()\n",
        "    self.monitor = monitor\n",
        "    self.save_path = save_path\n",
        "    self.bestScore = -np.Inf\n",
        "    self.bestLoss = np.Inf\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    score = logs.get(self.monitor)\n",
        "    loss = logs.get(\"val_loss\")\n",
        "    if score > self.bestScore or (score == self.bestScore and loss < self.bestLoss):\n",
        "      path = os.path.join(TASK, str(epoch+1))\n",
        "      os.makedirs(path)\n",
        "      self.model.save_weights(path+'/best_weights.h5')\n",
        "      self.bestScore = score\n",
        "      self.bestLoss = loss\n",
        "      print(\"\\nModel saved as the best model\")\n",
        "\n",
        "monitor = \"val_accuracy\"\n",
        "checkpoint = ModelCheckpoint(monitor, SAVED_MODELS_DIR)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ahIUftw-zwV",
        "cellView": "form"
      },
      "source": [
        "# @title Training\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    df_train[1],\n",
        "    epochs=EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    validation_data=(valid_dataset, df_val[1]),\n",
        "    callbacks=[checkpoint]\n",
        "\n",
        ")\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AhLSjbtFjzGb"
      },
      "source": [
        "# @title Retrieving Best Model\n",
        "import os \n",
        "list_of_dirs = os.listdir('/content/' + TASK)\n",
        " \n",
        "# using map() to\n",
        "# perform conversion\n",
        "final_list = list(map(int, list_of_dirs))\n",
        "best_model = max(final_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading and Evaluating the Best Model\n",
        "\n",
        "model_path =\"/content/\"+ TASK + '/' + str(best_model) + '/best_weights.h5'\n",
        "\n",
        "model.load_weights(model_path)\n",
        "\n",
        "model.evaluate(test_dataset, df_test[1])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5G8kp1pu7krY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tfuApSveAuo",
        "cellView": "form"
      },
      "source": [
        "#@title Removing the Checkpoints\n",
        "\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/' + TASK + '/')\n",
        "\n",
        "print(PROBE + ':')\n",
        "print(TASK + ' layer: ' + str(LAYER) + ' with size:' + MODEL_SIZE + '  is done')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}